{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "PCA (Análisis de Componentes Principales): PCA es una técnica de reducción de dimensionalidad que transforma el conjunto de datos original en un nuevo conjunto de características que son combinaciones lineales de las características originales. Se utiliza para reducir la dimensionalidad de los datos mientras se conserva la mayor cantidad posible de información.\n",
    "\n",
    "\n",
    "Selección de características univariadas: Estos métodos evalúan cada característica de forma independiente para seleccionar las más relevantes. Algunos ejemplos son:\n",
    "\n",
    "- VarianceThreshold: Elimina características con baja varianza.\n",
    "    \n",
    "        sklearn.feature_selección. VarianceThreshold()\n",
    "    \n",
    "- SelectKBest: Selecciona las k mejores características utilizando pruebas estadísticas univariadas.\n",
    "\n",
    "        sklearn.feature_selección. SeleccionarKBest()\n",
    "\n",
    "Selección de características basadas en modelos: Estos métodos utilizan un modelo para evaluar la importancia de cada característica y seleccionar las más relevantes. Algunos ejemplos son:\n",
    "\n",
    "- Lasso (L1 regularization): Penaliza los coeficientes de las características menos importantes, lo que puede conducir a la selección automática de características.\n",
    "\n",
    "        sklearn.linear_model.Lasso\n",
    "\n",
    "- RandomForestClassifier: Utiliza la importancia de las características calculada por el algoritmo de bosques aleatorios para seleccionar características.\n",
    "\n",
    "        sklearn.ensemble.RandomForestClassifier()\n",
    "\n",
    "- SelectFromModel: Selecciona automáticamente un subconjunto de características importantes basado en la importancia estimada por un modelo supervisado.\n",
    "\n",
    "Selección de características basadas en envoltura: Estos métodos evalúan diferentes subconjuntos de características utilizando el rendimiento de un modelo como criterio. Algunos ejemplos son:\n",
    "\n",
    "- SequentialFeatureSelector: Es un algoritmo de búsqueda exhaustiva que selecciona un subconjunto óptimo de características.\n",
    "\n",
    "        mlxtend.feature_selección. SequentialFeatureSelector()\n",
    "        \n",
    "- Recursive Feature Elimination (RFE): Realiza la eliminación recursiva de características ajustando el modelo con diferentes subconjuntos de características.\n",
    "\n",
    "\n",
    "\n",
    "Metodos estadisticos:\n",
    "\n",
    "- Vif: Selección de alta multicolinealidad: \n",
    "\n",
    "- Correlation: significancia estadística de las diferencias entre las muestras de las clases 0 y 1 utilizando la prueba de ANOVA.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REMEMBER: BECAUSE WE ARE DOING A PIPELINE FOR PREPROCESSING, IF WE ARE GOING TO IMPUTE MISSING VALUES OR NORMALIZE THE DATA, THE MODEL HAS TO BE TRAIN WITH THE TRAINING SET AND THE TRAIN-TEST SET (BOTH) HAVE TO BE TRANSFORM** - **DO THE TEST TRANSFORMATION BEFORE THE EVALUATION**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vif\n",
    "\n",
    "Vif realiza una selección de características basada en el VIF para eliminar características con alta multicolinealidad, lo que ayuda a mejorar la calidad de los modelos de aprendizaje automático al reducir la redundancia entre las características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import numpy as np\n",
    "\n",
    "class Vif(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "      X_train = X.copy()\n",
    "      self.dele = []\n",
    "\n",
    "      while True:\n",
    "          VIF = np.array([variance_inflation_factor(X_train, i) for i in range(X_train.shape[1])])\n",
    "\n",
    "          # delete the maximum one and repeat the count until the maximum value is lower than 5.\n",
    "          self.dele.append(np.argmax(VIF))\n",
    "          X_train = np.delete(X_train, self.dele[-1], axis=1)\n",
    "          if np.max(VIF) < 5:\n",
    "              break\n",
    "\n",
    "      return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.delete(X, self.dele, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vif2\n",
    "\n",
    "Vif2 realiza una selección de características basada en el VIF para eliminar características con alta multicolinealidad, lo que ayuda a mejorar la calidad de los modelos de aprendizaje automático al reducir la redundancia entre las características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import numpy as np \n",
    "\n",
    "class Vif2(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        self.dele = []\n",
    "        for _ in range(X.shape[1]):\n",
    "            vif = np.zeros(X.shape[1])\n",
    "            for i in range(X.shape[1]):\n",
    "                    if i in self.dele:\n",
    "                        continue\n",
    "                    vif[i] = variance_inflation_factor(X, i)\n",
    "\n",
    "            # delete the maximum one and repeat the count until the maximum value is lower than 5.\n",
    "            amax  = vif.argmax()\n",
    "            if vif[amax] < 5:\n",
    "                break\n",
    "            self.dele.append(amax)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.delete(X, self.dele, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diferencia entre Vif y Vif2:\n",
    "\n",
    "Ambas clases, Vif y Vif2, tienen el mismo propósito: realizar una selección de características basada en el factor de inflación de la varianza (VIF) para eliminar características altamente multicolineales. Sin embargo, difieren en su implementación interna:\n",
    "\n",
    "- Iteración:\n",
    "\n",
    "    Vif: Utiliza un bucle while para iterar y eliminar características con un VIF superior al umbral definido (5) hasta que todas las características restantes tengan un VIF inferior al umbral.\n",
    "\n",
    "    Vif2: Utiliza un bucle for anidado para calcular el VIF de cada característica individualmente y elimina la característica con el VIF más alto en cada iteración hasta que todas las características restantes tengan un VIF inferior al umbral.\n",
    "\n",
    "- Manipulación de datos:\n",
    "\n",
    "    Vif: Crea una copia de los datos de entrada X y luego realiza eliminaciones en esta copia a medida que se eliminan las características con alto VIF.\n",
    "\n",
    "    Vif2: Utiliza la matriz X original para calcular el VIF y luego elimina características de la matriz original.\n",
    "\n",
    "- Estructura de la clase:\n",
    "\n",
    "    Ambas clases implementan las interfaces BaseEstimator y TransformerMixin, lo que les permite ser utilizadas como estimadores y transformadores en scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "\n",
    "Correlation realiza una selección de características basada en la significancia estadística de las diferencias entre las muestras de las clases 0 y 1 utilizando la prueba de ANOVA. Las características que no cumplen con un cierto umbral de significancia estadística se eliminan del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "\n",
    "class Correlation(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, alpha=0.1):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "      X_train = X.copy()\n",
    "      y_train = y.copy()\n",
    "\n",
    "      self.dele = []\n",
    "      for i in range(X_train.shape[1]):\n",
    "        AnovaResults = f_oneway(X_train[y_train == 0, i], X_train[y_train == 1, i])\n",
    "\n",
    "        if AnovaResults[1] > self.alpha:\n",
    "          self.dele.append(i)\n",
    "\n",
    "\n",
    "      return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.delete(X, self.dele, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation2\n",
    "\n",
    "Correlation2 se utiliza para realizar una selección de características basada en la significancia estadística de las diferencias entre las características para diferentes clases de la variable objetivo utilizando la prueba de ANOVA. Las características que no cumplen con un cierto umbral de significancia estadística se eliminan del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Correlation2(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, alpha=0.1):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        _, a_p      = f_oneway(X[y == 0, :], X[y == 1, :])\n",
    "        self.dele   = np.flatnonzero(a_p > self.alpha)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.delete(X, self.dele, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diferencias entre Correlation y Correlation2:\n",
    "\n",
    "\n",
    "- Método de selección de características:\n",
    "\n",
    "    Correlation: Utiliza el método ANOVA (Análisis de varianza) para calcular la importancia estadística de cada característica en relación con la variable objetivo. Luego, selecciona las características cuyos valores de p de ANOVA superan un umbral predefinido (alpha) para eliminarlas.\n",
    "\n",
    "    Correlation2: Utiliza una estrategia diferente para calcular la importancia de cada característica en relación con la variable objetivo. Se basa en el método de eliminación iterativa, donde en cada iteración se calcula el VIF (Factor de inflación de la varianza) para cada característica y se elimina la característica con el VIF más alto hasta que ningún VIF sea superior a un umbral predefinido (5).\n",
    "\n",
    "- Manipulación de datos:\n",
    "\n",
    "    Correlation: No modifica directamente los datos de entrada X, sino que utiliza las funciones f_oneway y np.delete para calcular la importancia de las características y eliminar las características seleccionadas.\n",
    "\n",
    "    Correlation2: Modifica los datos de entrada X durante el proceso de ajuste y transformación, eliminando directamente las características con alto VIF.\n",
    "\n",
    "- Estructura de la clase:\n",
    "\n",
    "    Ambas clases implementan las interfaces BaseEstimator y TransformerMixin de scikit-learn, lo que les permite ser utilizadas como estimadores y transformadores en pipelines de scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pipeline\n",
    "\n",
    "\n",
    "Esta pipeline es el proceso de feature selection que se debe implementar antes de aplicar los modelos. Aqui se puede especificar el numero de feature selection, podiendo implementar uno o tantos como se quieran y en el orden que se quiera. Anteriormente se han definido funciones especificas pero aqui se especifican mas modelos de feature selection que ya estan implementados y cargados en python como IterativeImputer - para eliminar nulos - o StandardScaler - o PCA.\n",
    "\n",
    "En este ejemplo, como solo estamos hablando de feature selection sobraria los dos primeros metodos, pero por su necesario uso en el origen se dejan para demostracion de como se deberian implementar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PIPELINE ###\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('impute', IterativeImputer(random_state = 42)), #strategy to substitude the null values: mean\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('vif', Vif2()),\n",
    "    ('correlation', Correlation2()),\n",
    "    ('pca', PCA()),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un ejemplo de la implementacion del pipeline en un modelo seria, utilizando un Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100483869) # reproducibility\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Put together model\n",
    "clf_LR = Pipeline ([\n",
    "    ( 'preprocessor', pipeline ),\n",
    "    ('classifier', model)])\n",
    "\n",
    "clf_LR_grid = clf_LR.fit(X=X_train0, y=y_train0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder hacer la evaluacion de el modelo, deberemos aplicar esta normalizacion al train y test antes de realizar la evaluacion asi que:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SCORING OF THE MODEL WITH THE TRAINING SET ####\n",
    "\n",
    "#retrieve the best parameters found, and an instance of the classifier already trained with these parameter\n",
    "classifier = clf_LR_grid.best_estimator_\n",
    "best_params = clf_LR_grid.best_params_\n",
    "\n",
    "\n",
    "print(\"The best hyperparameters are:\")\n",
    "print(best_params)\n",
    "\n",
    "\n",
    "## PREDICTION AND SCORING OVER THE TEST DATA ##\n",
    "print(\"Training set accuracy {0:.2f} %\".format(classifier.score(xs_train, ys_train)*100))\n",
    "print(\"Test set accuracy {0:.2f} %\".format(classifier.score(xs_test, ys_test)*100))\n",
    "\n",
    "pred_test = classifier.predict(xs_test)\n",
    "print('\\n')\n",
    "print(\"Test set balanced accuracy {0:.2f} %\".format(balanced_accuracy_score(ys_test,pred_test)*100))\n",
    "\n",
    "acc_rf = classifier.score(xs_test, ys_test)\n",
    "print(acc_rf)\n",
    "balanced_rf = balanced_accuracy_score(ys_test,pred_test)\n",
    "print(balanced_rf)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
