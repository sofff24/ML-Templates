{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Models:\n",
    "\n",
    "- Logistic Regression\n",
    "- SVM\n",
    "- Decision Tress\n",
    "- Random Forest\n",
    "- Gradient Boosting Machine\n",
    "- ANN\n",
    "- KNN\n",
    "- Naive Bayes\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "En este python, se aplicaran los modelos a traves de una pipeline. De este modo se odra aplicar una pipeline de preprocesamiento que incluya eliminacion de valores nulos, normalizacion y algunos modelos de feature selection, de este modo podemos rapidamente aplicar distintoas fases.\n",
    "\n",
    "\n",
    "**REMEMBER: BECAUSE WE ARE DOING A PIPELINE FOR PREPROCESSING, IF WE ARE GOING TO IMPUTE MISSING VALUES OR NORMALIZE THE DATA, THE MODEL HAS TO BE TRAIN WITH THE TRAINING SET AND THE TRAIN-TEST SET (BOTH) HAVE TO BE TRANSFORM** - **DO THE TEST TRANSFORMATION BEFORE THE EVALUATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BASIC MODEL APPLICATION\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('impute', IterativeImputer(random_state = 42)), # substitude the null values: mean\n",
    "    ('scaler', StandardScaler())                     # to normalize the data\n",
    "                                                     # you can add feature selection like PCA here\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(100483869) # reproducibility\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Put together model\n",
    "model_p = Pipeline ([\n",
    "    ( 'preprocessor', pipeline ),\n",
    "    ('classifier', model)])\n",
    "\n",
    "LR_model = model_p.fit(X=X_train0, y=y_train0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COMPLEX MODEL\n",
    "# In case we want to add hyperparameter tuning and Cross-validation we can add it like this, \n",
    "# we have to add it to the basic model without the previoud fit - \n",
    "# WE HAVE TO ERASE LINE: LR_model = model_p.fit(X=X_train0, y=y_train0)\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model_p = Pipeline ([\n",
    "    ( 'preprocessor', pipeline ),\n",
    "    ('classifier', model)])\n",
    "\n",
    "#### HYPER-PARAMETERS  #######\n",
    "# ranges for gamma and C\n",
    "param_grid = {\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1.0, 10.0, 100],\n",
    "        'classifier__max_iter': [100, 1000, 10000]}\n",
    "\n",
    "# Inner CV\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=100483869)\n",
    "\n",
    "\n",
    "# instantiation of the grid of hyperparameters that will be searched with cross validation\n",
    "\n",
    "grid_search = GridSearchCV(model_p, #model\n",
    "                   param_grid, # rango of value of the hyper-parameter to evaluate\n",
    "                   scoring='f1',\n",
    "                   cv=cv,\n",
    "                   n_jobs=-1,\n",
    "                   verbose=3) #-1 means use all processors\n",
    "\n",
    "\n",
    "\n",
    "#START OF THE RECORD\n",
    "LR_model_grid = grid_search.fit(X=X_train0, y=y_train0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BASIC MODEL APPLICATION\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('impute', IterativeImputer(random_state = 42)), # substitude the null values: mean\n",
    "    ('scaler', StandardScaler())                     # to normalize the data\n",
    "                                                     # you can add feature selection like PCA here\n",
    "    ])\n",
    "\n",
    "\n",
    "model = SVC()\n",
    "\n",
    "# Put together model\n",
    "model_p = Pipeline ([\n",
    "    ( 'preprocessor', pipeline ),\n",
    "    ('classifier', model)])\n",
    "\n",
    "SVM_model = model_p.fit(X=X_train0, y=y_train0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COMPLEX MODEL\n",
    "# In case we want to add hyperparameter tuning and Cross-validation we can add it like this, \n",
    "# we have to add it to the basic model without the previoud fit - \n",
    "# WE HAVE TO ERASE LINE: SVM_model = model_p.fit(X=X_train0, y=y_train0)\n",
    "\n",
    "model = SVC()\n",
    "\n",
    "# Put together model\n",
    "model_p = Pipeline ([\n",
    "    ( 'preprocessor', pipeline ),\n",
    "    ('classifier', model)])\n",
    "\n",
    "#### HYPER-PARAMETERS  #######\n",
    "# Search space of SVM hyperparameters\n",
    "v_gamma = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "v_C = [0.001,.01, .1, 1, 10, 100, 1000]\n",
    "# Grid of hyperparameters\n",
    "param_grid = [\n",
    "  {'classifier__C': v_C,\n",
    "   'classifier__gamma': v_gamma,\n",
    "   'classifier__kernel': ['rbf']},\n",
    "  ]\n",
    "\n",
    "# Inner CV\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=100483869)\n",
    "\n",
    "\n",
    "# instantiation of the grid of hyperparameters that will be searched with cross validation\n",
    "\n",
    "grid_search = GridSearchCV(model_p, #model\n",
    "                   param_grid, # rango of value of the hyper-parameter to evaluate\n",
    "                   scoring='f1',\n",
    "                   cv=cv,\n",
    "                   n_jobs=-1,\n",
    "                   verbose=2) #-1 means use all processors\n",
    "\n",
    "\n",
    "\n",
    "#START OF THE RECORD\n",
    "SVM_model_grid = grid_search.fit(X=X_train0, y=y_train0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BASIC MODEL\n",
    "np.random.seed(100483869) # reproducibility\n",
    "\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "model_p = Pipeline ([\n",
    "    ( 'preprocessor', pipeline ),\n",
    "    ('classifier', model)])\n",
    "\n",
    "#START OF THE RECORD\n",
    "RF_model_grid = model_p.fit(X=X_train0, y=y_train0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# COMPLEX MODEL\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "model_p = Pipeline ([\n",
    "    ( 'preprocessor', pipeline ),\n",
    "    ('classifier', model)])\n",
    "\n",
    "\n",
    "# Search space of SVM hyperparameters\n",
    "\n",
    "param_grid = [\n",
    "  {'classifier__max_depth': list(range(2,10,1)),\n",
    "   'classifier__min_samples_split': list(range(2,10,1)) }   ]\n",
    "\n",
    "\n",
    "# Inner CV\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=100483869)\n",
    "\n",
    "\n",
    "# Put together model\n",
    "clf_RF = Pipeline ([\n",
    "    ( 'preprocessor', pipeline ),\n",
    "    ('classifier', model)])\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(model_p, #model\n",
    "                   param_grid, # rango of value of the hyper-parameter to evaluate\n",
    "                   scoring='f1',\n",
    "                   cv=cv,\n",
    "                   n_jobs=-1,\n",
    "                   verbose=2) #-1 means use all processors\n",
    "\n",
    "\n",
    "#START OF THE RECORD\n",
    "RF_model_grid = grid_search.fit(X=X_train0, y=y_train0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BASIC ##\n",
    "\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "model_p = Pipeline ([\n",
    "    ( 'preprocessor', pipeline ),\n",
    "    ('classifier', model)])\n",
    "\n",
    "\n",
    "#START OF THE RECORD\n",
    "GB_model = model_p.fit(X=X_train0, y=y_train0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COMPLEX MODEL ##\n",
    "\n",
    "np.random.seed(100483869) # reproducibility\n",
    "\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "model_p = Pipeline ([\n",
    "    ( 'preprocessor', pipeline ),\n",
    "    ('classifier', model)])\n",
    "\n",
    "# Search space of SVM hyperparameters\n",
    "\n",
    "param_grid = {\n",
    "        'classifier__min_samples_leaf': np.linspace(20, 40, 3, dtype=int),\n",
    "        'classifier__max_leaf_nodes': np.linspace(20, 60, 5, dtype=int) }\n",
    "\n",
    "\n",
    "# Inner CV\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=100483869)\n",
    "\n",
    "\n",
    "# Put together model\n",
    "clf_GB = Pipeline ([\n",
    "    ( 'preprocessor', pipeline ),\n",
    "    ('classifier', model)])\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(model_p, #model\n",
    "                   param_grid, # rango of value of the hyper-parameter to evaluate\n",
    "                   scoring='f1',\n",
    "                   cv=cv,\n",
    "                   n_jobs=-1,\n",
    "                   verbose=2) #-1 means use all processors\n",
    "\n",
    "\n",
    "#START OF THE RECORD\n",
    "GB_model_grid = grid_search.fit(X=X_train0, y=y_train0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_pip = Pipeline([('scaler', StandardScaler()), \n",
    "                    ('model', MLPClassifier(activation ='logistic', # here we are defining we are using sigmoid - change it for other \n",
    "                                             max_iter = 300, \n",
    "                                             random_state = 42))\n",
    "                    ])\n",
    "\n",
    "#### HYPER-PARAMETERS  #######\n",
    "param_grid = [\n",
    "  {'model__alpha': [0.0001, 0.001, 0.01]} ]\n",
    "\n",
    "# instantiation of the grid of hyperparameters that will be searched with cross validation\n",
    "mlp_baseline = HalvingGridSearchCV(\n",
    "        mlp_pip, # pipeline\n",
    "        param_grid, # grid of hyperparameters\n",
    "        n_jobs = -1,\n",
    "        verbose = 2)\n",
    "\n",
    "\n",
    "#grid_SVM = SVC(gamma='auto') # without hyper-parameter\n",
    "\n",
    "mlp_baseline.fit(xs_train, ys_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SCORING OF THE MODEL WITH THE TRAINING SET ####\n",
    "\n",
    "#retrieve the best parameters found, and an instance of the classifier already trained with these parameter\n",
    "classifier = LR_model_grid.best_estimator_\n",
    "best_params = LR_model_grid.best_params_\n",
    "\n",
    "\n",
    "print(\"The best hyperparameters are:\")\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions - accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EVALUATION FOR A CLASSIFICATION METHOD ##\n",
    "## PREDICTION AND SCORING OVER THE TEST DATA ##\n",
    "print(\"Training set accuracy {0:.2f} %\".format(classifier.score(xs_train, ys_train)*100))\n",
    "print(\"Test set accuracy {0:.2f} %\".format(classifier.score(xs_test, ys_test)*100))\n",
    "\n",
    "pred_test = classifier.predict(xs_test)\n",
    "print('\\n')\n",
    "print(\"Test set balanced accuracy {0:.2f} %\".format(balanced_accuracy_score(ys_test,pred_test)*100))\n",
    "\n",
    "acc_rf = classifier.score(xs_test, ys_test)\n",
    "print(acc_rf)\n",
    "balanced_rf = balanced_accuracy_score(ys_test,pred_test)\n",
    "print(balanced_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('dark')\n",
    "target = [x for x in r['genre']]\n",
    "disp = ConfusionMatrixDisplay.from_predictions(ys_test, pred_test, display_labels = target, cmap = custom_colormap, text_kw = {\"color\": \"black\"})\n",
    "plt.title('Confusion Matrix - Random Forest')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
