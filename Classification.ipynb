{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Models:\n",
    "\n",
    "- Logistic Regression\n",
    "- SVM\n",
    "- Decision Tress\n",
    "- Random Forest\n",
    "- Gradient Boosting Machine\n",
    "- MLP\n",
    "- K-MEANS\n",
    "- ANN\n",
    "- KNN\n",
    "- Naive Bayes\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "En este python, se aplicaran los modelos a traves de una pipeline. De este modo se odra aplicar una pipeline de preprocesamiento que incluya eliminacion de valores nulos, normalizacion y algunos modelos de feature selection, de este modo podemos rapidamente aplicar distintoas fases.\n",
    "\n",
    "\n",
    "**REMEMBER: BECAUSE WE ARE DOING A PIPELINE FOR PREPROCESSING, IF WE ARE GOING TO IMPUTE MISSING VALUES OR NORMALIZE THE DATA, THE MODEL HAS TO BE TRAIN WITH THE TRAINING SET AND THE TRAIN-TEST SET (BOTH) HAVE TO BE TRANSFORM** - **DO THE TEST TRANSFORMATION BEFORE THE EVALUATION**\n",
    "\n",
    "\n",
    "\n",
    "In addition we have the following points:\n",
    "\n",
    "- How to save a model\n",
    "- EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BASIC MODEL APPLICATION\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('impute', IterativeImputer(random_state = 42)), # substitude the null values: mean\n",
    "    ('scaler', StandardScaler())                     # to normalize the data\n",
    "                                                     # you can add feature selection like PCA here\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(100483869) # reproducibility\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Put together model\n",
    "model_p = Pipeline ([\n",
    "    ( 'preprocessor', pipeline ),\n",
    "    ('classifier', model)])\n",
    "\n",
    "LR_model = model_p.fit(X=X_train0, y=y_train0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COMPLEX MODEL\n",
    "# In case we want to add hyperparameter tuning and Cross-validation we can add it like this, \n",
    "# we have to add it to the basic model without the previoud fit - \n",
    "# WE HAVE TO ERASE LINE: LR_model = model_p.fit(X=X_train0, y=y_train0)\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model_p = Pipeline ([\n",
    "    ( 'preprocessor', pipeline ),\n",
    "    ('classifier', model)])\n",
    "\n",
    "#### HYPER-PARAMETERS  #######\n",
    "# ranges for gamma and C\n",
    "param_grid = {\n",
    "        'classifier__C': [0.001, 0.01, 0.1, 1.0, 10.0, 100],\n",
    "        'classifier__max_iter': [100, 1000, 10000]}\n",
    "\n",
    "# Inner CV\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=100483869)\n",
    "\n",
    "\n",
    "# instantiation of the grid of hyperparameters that will be searched with cross validation\n",
    "\n",
    "grid_search = GridSearchCV(model_p, #model\n",
    "                   param_grid, # rango of value of the hyper-parameter to evaluate\n",
    "                   scoring='f1',\n",
    "                   cv=cv,\n",
    "                   n_jobs=-1,\n",
    "                   verbose=3) #-1 means use all processors\n",
    "\n",
    "\n",
    "\n",
    "#START OF THE RECORD\n",
    "LR_model_grid = grid_search.fit(X=X_train0, y=y_train0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BASIC MODEL APPLICATION\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('impute', IterativeImputer(random_state = 42)), # substitude the null values: mean\n",
    "    ('scaler', StandardScaler())                     # to normalize the data\n",
    "                                                     # you can add feature selection like PCA here\n",
    "    ])\n",
    "\n",
    "\n",
    "model = SVC()\n",
    "\n",
    "# Put together model\n",
    "model_p = Pipeline ([\n",
    "    ( 'preprocessor', pipeline ),\n",
    "    ('classifier', model)])\n",
    "\n",
    "SVM_model = model_p.fit(X=X_train0, y=y_train0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COMPLEX MODEL\n",
    "# In case we want to add hyperparameter tuning and Cross-validation we can add it like this, \n",
    "# we have to add it to the basic model without the previoud fit - \n",
    "# WE HAVE TO ERASE LINE: SVM_model = model_p.fit(X=X_train0, y=y_train0)\n",
    "\n",
    "model = SVC()\n",
    "\n",
    "# Put together model\n",
    "model_p = Pipeline ([\n",
    "    ( 'preprocessor', pipeline ),\n",
    "    ('classifier', model)])\n",
    "\n",
    "#### HYPER-PARAMETERS  #######\n",
    "# Search space of SVM hyperparameters\n",
    "v_gamma = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "v_C = [0.001,.01, .1, 1, 10, 100, 1000]\n",
    "# Grid of hyperparameters\n",
    "param_grid = [\n",
    "  {'classifier__C': v_C,\n",
    "   'classifier__gamma': v_gamma,\n",
    "   'classifier__kernel': ['rbf']},\n",
    "  ]\n",
    "\n",
    "# Inner CV\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=100483869)\n",
    "\n",
    "\n",
    "# instantiation of the grid of hyperparameters that will be searched with cross validation\n",
    "\n",
    "grid_search = GridSearchCV(model_p, #model\n",
    "                   param_grid, # rango of value of the hyper-parameter to evaluate\n",
    "                   scoring='f1',\n",
    "                   cv=cv,\n",
    "                   n_jobs=-1,\n",
    "                   verbose=2) #-1 means use all processors\n",
    "\n",
    "\n",
    "\n",
    "#START OF THE RECORD\n",
    "SVM_model_grid = grid_search.fit(X=X_train0, y=y_train0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create a pipeline with a scaler and a Decision Tree classifier\n",
    "dt_pip = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('model', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Define hyperparameters for the Decision Tree classifier\n",
    "param_grid = [\n",
    "    {'model__max_depth': [None, 10, 20, 30], \n",
    "     'model__min_samples_split': [2, 5, 10], \n",
    "     'model__min_samples_leaf': [1, 2, 4]}\n",
    "]\n",
    "\n",
    "# Instantiate the HalvingGridSearchCV for hyperparameter tuning\n",
    "dt_baseline = HalvingGridSearchCV(\n",
    "    dt_pip,         # pipeline\n",
    "    param_grid,     # grid of hyperparameters\n",
    "    n_jobs=-1,      # use all available CPU cores\n",
    "    verbose=2       # level of verbosity\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "dt_baseline.fit(xs_train, ys_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BASIC MODEL\n",
    "np.random.seed(100483869) # reproducibility\n",
    "\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "model_p = Pipeline ([\n",
    "    ( 'preprocessor', pipeline ),\n",
    "    ('classifier', model)])\n",
    "\n",
    "#START OF THE RECORD\n",
    "RF_model_grid = model_p.fit(X=X_train0, y=y_train0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# COMPLEX MODEL\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "model_p = Pipeline ([\n",
    "    ( 'preprocessor', pipeline ),\n",
    "    ('classifier', model)])\n",
    "\n",
    "\n",
    "# Search space of SVM hyperparameters\n",
    "\n",
    "param_grid = [\n",
    "  {'classifier__max_depth': list(range(2,10,1)),\n",
    "   'classifier__min_samples_split': list(range(2,10,1)) }   ]\n",
    "\n",
    "\n",
    "# Inner CV\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=100483869)\n",
    "\n",
    "\n",
    "# Put together model\n",
    "clf_RF = Pipeline ([\n",
    "    ( 'preprocessor', pipeline ),\n",
    "    ('classifier', model)])\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(model_p, #model\n",
    "                   param_grid, # rango of value of the hyper-parameter to evaluate\n",
    "                   scoring='f1',\n",
    "                   cv=cv,\n",
    "                   n_jobs=-1,\n",
    "                   verbose=2) #-1 means use all processors\n",
    "\n",
    "\n",
    "#START OF THE RECORD\n",
    "RF_model_grid = grid_search.fit(X=X_train0, y=y_train0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BASIC ##\n",
    "\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "model_p = Pipeline ([\n",
    "    ( 'preprocessor', pipeline ),\n",
    "    ('classifier', model)])\n",
    "\n",
    "\n",
    "#START OF THE RECORD\n",
    "GB_model = model_p.fit(X=X_train0, y=y_train0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COMPLEX MODEL ##\n",
    "\n",
    "np.random.seed(100483869) # reproducibility\n",
    "\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "model_p = Pipeline ([\n",
    "    ( 'preprocessor', pipeline ),\n",
    "    ('classifier', model)])\n",
    "\n",
    "# Search space of SVM hyperparameters\n",
    "\n",
    "param_grid = {\n",
    "        'classifier__min_samples_leaf': np.linspace(20, 40, 3, dtype=int),\n",
    "        'classifier__max_leaf_nodes': np.linspace(20, 60, 5, dtype=int) }\n",
    "\n",
    "\n",
    "# Inner CV\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=100483869)\n",
    "\n",
    "\n",
    "# Put together model\n",
    "clf_GB = Pipeline ([\n",
    "    ( 'preprocessor', pipeline ),\n",
    "    ('classifier', model)])\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(model_p, #model\n",
    "                   param_grid, # rango of value of the hyper-parameter to evaluate\n",
    "                   scoring='f1',\n",
    "                   cv=cv,\n",
    "                   n_jobs=-1,\n",
    "                   verbose=2) #-1 means use all processors\n",
    "\n",
    "\n",
    "#START OF THE RECORD\n",
    "GB_model_grid = grid_search.fit(X=X_train0, y=y_train0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_pip = Pipeline([('scaler', StandardScaler()), \n",
    "                    ('model', MLPClassifier(activation ='logistic', # here we are defining we are using sigmoid - change it for other \n",
    "                                             max_iter = 300, \n",
    "                                             random_state = 42))\n",
    "                    ])\n",
    "\n",
    "#### HYPER-PARAMETERS  #######\n",
    "param_grid = [\n",
    "  {'model__alpha': [0.0001, 0.001, 0.01]} ]\n",
    "\n",
    "# instantiation of the grid of hyperparameters that will be searched with cross validation\n",
    "mlp_baseline = HalvingGridSearchCV(\n",
    "        mlp_pip, # pipeline\n",
    "        param_grid, # grid of hyperparameters\n",
    "        n_jobs = -1,\n",
    "        verbose = 2)\n",
    "\n",
    "\n",
    "#grid_SVM = SVC(gamma='auto') # without hyper-parameter\n",
    "\n",
    "mlp_baseline.fit(xs_train, ys_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-MEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR K=4\n",
    "\n",
    "kmeans_pip = Pipeline([('scaler', StandardScaler()), \n",
    "                    ('model', KMeans(n_clusters = 4, random_state = 42))\n",
    "                    ])\n",
    "\n",
    "param_grid = [\n",
    "  {'model__tol': [1e-4, 1e-3, 1e-2],\n",
    "   'model__n_init': [10, 25, 50] }]\n",
    "\n",
    "# model\n",
    "params = {\"model__n_clusters\": 4}\n",
    "kmeans_pip.set_params(**params)\n",
    "\n",
    "kmean = HalvingGridSearchCV(\n",
    "             kmeans_pip, # pipeline\n",
    "             param_grid, # grid of hyperparameters\n",
    "             n_jobs  = -1,\n",
    "             verbose = 2)\n",
    "\n",
    "kmean.fit(x_train_array, y_train_array)\n",
    "\n",
    "\n",
    "# SEPARATION OF THE GROUPS\n",
    "# TRAIN\n",
    "clusters_train = kmean.predict(x_train_array)\n",
    "\n",
    "# TEST\n",
    "clusters_test = kmean.predict(x_test_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT OF THE NEW CLUSTERS:\n",
    "pd.DataFrame(clusters_train)[0].value_counts()\n",
    "dic = {'1':6315, '2': 7406, '3': 7249, '4':1727}\n",
    "dic\n",
    "names = list(dic.keys())\n",
    "values = list(dic.values())\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.bar(range(len(dic)), values, tick_label=names, color='#13414e')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"mlp_km6_selected{}.pkl\".format(i+1)\n",
    "with open(os.path.join(\"model_columns_selected\", filename), 'wb') as f:\n",
    "pickle.dump(mlp_km, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SCORING OF THE MODEL WITH THE TRAINING SET ####\n",
    "\n",
    "#retrieve the best parameters found, and an instance of the classifier already trained with these parameter\n",
    "classifier = LR_model_grid.best_estimator_\n",
    "best_params = LR_model_grid.best_params_\n",
    "\n",
    "\n",
    "print(\"The best hyperparameters are:\")\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions - accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EVALUATION FOR A CLASSIFICATION METHOD ##\n",
    "## PREDICTION AND SCORING OVER THE TEST DATA ##\n",
    "print(\"Training set accuracy {0:.2f} %\".format(classifier.score(xs_train, ys_train)*100))\n",
    "print(\"Test set accuracy {0:.2f} %\".format(classifier.score(xs_test, ys_test)*100))\n",
    "\n",
    "pred_test = classifier.predict(xs_test)\n",
    "print('\\n')\n",
    "print(\"Test set balanced accuracy {0:.2f} %\".format(balanced_accuracy_score(ys_test,pred_test)*100))\n",
    "\n",
    "acc_rf = classifier.score(xs_test, ys_test)\n",
    "print(acc_rf)\n",
    "balanced_rf = balanced_accuracy_score(ys_test,pred_test)\n",
    "print(balanced_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PRETTY TABLE\n",
    "\n",
    "accuracies = [acc_rf, acc_svm, acc_gb]\n",
    "balanced = [balanced_rf, balanced_svm, balanced_gb]\n",
    "modelos = ['Random Forest','SVM','Gradient Boosting']\n",
    "\n",
    "results = {'Accuracy': accuracies, 'Balanced Accuracy': balanced}\n",
    "\n",
    "pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# TABLE FOR LATEX #\n",
    "print(pd.DataFrame(results).to_latex(index=False,\n",
    "                  formatters={\"name\": str.upper},\n",
    "                  float_format=\"{:.3f}\".format))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('dark') #this is the style\n",
    "target = [x for x in r['genre']]\n",
    "disp = ConfusionMatrixDisplay.from_predictions(ys_test, pred_test, \n",
    "                                               display_labels = target, # LABELS\n",
    "                                               cmap = custom_colormap,  # COLORS OF THE GRAPH\n",
    "                                               text_kw = {\"color\": \"black\"}) # COLORS OF THE NUMBERS \n",
    "plt.title('Confusion Matrix - Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## HOW TO CHANGE COLORS #\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "def create_custom_colormap(colors, positions=None):\n",
    "    if positions is None:\n",
    "        positions = np.linspace(0, 1, len(colors))\n",
    "    \n",
    "    return LinearSegmentedColormap.from_list(\"\", list(zip(positions, colors)))\n",
    "\n",
    "\n",
    "### DARK ORANGE -> #d95d39\n",
    "### MIDDLE ORANGE -> #f18805\n",
    "### LIGHT ORANGE -> #f0a202\n",
    "### LIGHT 2 ORANGE -> #fdba43\n",
    "### BLUE -> #13414e\n",
    "\n",
    "\n",
    "colors = ['#fdba43', '#f18805', '#d95d39']  # This is a simple green-yellow-red colormap\n",
    "custom_colormap = create_custom_colormap(colors)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
